---
title: "EDS 222: Week 3: In-class Lab"
author: "Paloma Cartwright"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
---

# Section 0: Getting set up

From last week, you should already have done the following:

1. Create a _Labs/_ folder where you will store all your lab materials for EDS 222. 
2. Download `_common.R` and put it in the _Labs/_ folder.
3. Install the following packages, if they are not already installed:^[Note: This will take a minute or two!]

```{r setup, include=FALSE, echo = FALSE, eval = FALSE}
#suppressMessages({
#  install.packages( "tidymodels", "gghighlight", "glue", "ggmosaic", "ggridges", "gridExtra", "infer", "janitor", "knitr", "kableExtra", "maps", "openintro", "patchwork", "quantreg", "tidyverse", "scales", "skimr", "caret", "palmerpenguins", "survival", "waffle", "ggrepel", "ggpubr", "openintro")
#})
```

4. Load all the packages you need, plus the `_common.R` source file.^[Note: If you get a `namespace load` error for `tidyverse`, you want to remove the problem package, re install it, and then load `tidyverse` again.]

```{r, echo = FALSE, eval = TRUE, include=TRUE, warning=FALSE}
# For labs, we want to see all our code
knitr::opts_chunk$set(echo = TRUE)
# You probably already have these packages installed, so let's just load them
library(tidyverse)
library(readr)
library(gt)
library(openintro)
library(ggplot2)
library(modelr)
library(here)
library(patchwork)

options(scipen = 999) # disable scientific notation
# This runs the script _common.R, which loads all the packages we'll need for today and does some extra stuff we won't really use, at least for now.
source(here::here("labs","_common.R"))
```

# Section 1: Visualizing Scatterplots

Using similar concepts from a histogram or a density plot, we can use a scatter plot to visualize the relationship between two variables. It is **always** a good idea to look at a scatter plot when trying to establish the relationship between two variables. A scatter plot is a simple two-dimensional plot in which the two coordinates of each dot represent the values of two variables measured on a single observation. The **dependent** or response variable is typically on the vertical axis and the **independent** or explanatory variable on the horizontal axis.

Here we will use a data frame on possums and we will first try to see if (i) the total length of a possum is associated with the length of a possum's tail; (ii) the length of a possum's head is associated with the the length of possum's tail. Don't ask me why we might care about this, but I'm sure there are biologists who do! We just want some practice with `ggplot()`. 

Split into groups of 2-3 and work on the following for about 10 minutes:

The data are already loaded in `R`, and can be called using `data = possum`. Take a look at your variable names and make a scatter plot showing  (i) total length as a function of tail length and (ii) head length as a function of tail length. That is, treat tail length as your independent variable. 

1. Make the two scatter plots described above.

```{r}

tail_total <- ggplot(data = possum, aes(x = tail_l, y = total_l )) +
  geom_point() +
  labs(x = "Tail Length (cm)", 
       y = "Total Length (cm)") +
  theme_classic()

tail_head <- ggplot(data = possum, aes(x = tail_l, y = head_l)) +
  geom_point() +
  labs(x = "Tail Length (cm)", 
       y = "Head Length (cm)") + 
  theme_classic()

tail_total 
tail_head
```

2. Discuss whether these two variables appear correlated, what sign you anticipate the correlation to be, and the strength of the correlation you expect.

Both of these variables are positively correlated. The tail length and body leangth are pretty strongly positively correlated, but the head length and tail length are a little less positively correlated. 

# Section 2: Calculating Correlations

Visualizing values of the correlation between two quantities from their scatter plot can be very difficult. Research has shown that people’s perception of the strength of these relationships can be influenced by design choices like the size of the graph and the aspect ratio,^[The _aspect ratio_ is the ratio of the width to the height of a figure or image.] among other features. Hence, to more objectively report the relationship between two variables, we use correlation, computing the "correlation coefficient" $r$.^[Note: we just called this "correlation" in lecture, but people use "correlation" and "correlation coefficient" interchangeably. In most cases, this refers to the Pearson's correlation coefficient, as defined in lecture. There are other forms of correlation that are less common, such as rank correlation, but we won't use them in this class.] Here, we will compute correlation coefficients for the two possum relationships plotted above.

**Exercise:**

1. Use the plots from section 1 to write your best estimate of the correlation coefficient between each of the two examples in section 1. 
  (i) the length of a possum's total length and the length of possum tail: 0.72
  (ii) the length of a possum's head and the length of possum tail: 0.46

2. Use the `cor()` function in `R` to calculate the correlation coefficient between the two variables in (i) and the two in (ii). Prize^[Prize in the form of honor and admiration.] to whoever's guess was closest! 

```{r}

possum %>% 
  summarize(total_tail = cor(total_l, tail_l)) %>% 
  gt()

cor(x = possum$total_l, y = possum$tail_l) #defualt is Pearson's method

cor(x = possum$tail_l, y = possum$head_l)
```



# Section 3: Simple Linear Regression

## Perform Simple Linear Regression

To estimate the linear relationship between the variables explored above, we'll use `lm()` to fit a regression model using Ordinary Least Squares (OLS). The `lm()` function has two required arguments:

(i) a formula that indicates which variable is the dependent variable and which is the independent variable;
(ii) a data argument that identifies the data frame for the variables in (i).

First let's consider the relationship between total length and tail length. We assume there is a linear population relationship, specified as:

$$\text{possum total length} = \beta_0 + \beta_1 \text{possum tail length} + \epsilon$$

**Exercise:**

1. Use `lm()` to estimate $\hat\beta_0$ and $\hat\beta_1$ using this sample of data. Use `summary(lm())` or `gt()` to visualize the regression results.

```{r}

summary(lm(total_l ~ tail_l, data = possum))
lm(total_l ~ tail_l, data = possum) %>% 
  tidy %>% 
  gt()

```


2. Interpret your two coefficients, paying careful attention to units. 

The intercept of total length and tail length is at 41.04. The slope is 1.24. This is pretty logical. 
$\hat\beta_0$ is 41.04 and $\hat\beta_1$ is 1.24

For every 1 centimeter increase in tail length, the total length increases by 1.24. 

3. Is your estimate of $\beta_0$ directly interpretable? What might you be concerned about when interpreting $\beta_0$? 

The estimate of $\beta_0$ is interpretable but because possums are born with tails and the intercept assumes that the tail length is 0 so it is not really helpful. 

4. Does your model suggest anything about the relationship tail length and body length^[Note that the "total" length is the sum of tail length plus body length.]? Why or why not? 

Because total length is the sum of tail length plus body length, and the rate of change in the total length is 1.24, that means that there is a 0.24 dependence on the increase in body length to ensure that the total length increases by the slope found. This means that there is a positive correlation between tail length and body length. 

5. [If we have time] Repeat the above steps for the relationship between possum tail length and possum head length. Can you learn anything from directly comparing magnitudes of $\hat\beta_1$ across these two regressions?



**Answers:**

## Visualize Regression [if we have time]

Here, we visualize our simple linear regression models above using the scatter plots from Section 1.

**Exercise:** 

Use the `geom_smooth()` function with method argument set to `lm`, which stands for “linear model”, to add the best fit OLS line to your scatter plots from Section 1. Use the option `se = FALSE` to turn off estimates of uncertainty. Don't worry, we'll dig into those in a week.

```{r}
tail_total <- ggplot(data = possum, aes(x = tail_l, y = total_l )) +
  geom_point() +
  labs(x = "Tail Length (cm)", 
       y = "Total Length (cm)") +
  theme_classic() +
  geom_smooth(method = "lm", se=FALSE)

tail_head <- ggplot(data = possum, aes(x = tail_l, y = head_l)) +
  geom_point() +
  labs(x = "Tail Length (cm)", 
       y = "Head Length (cm)") + 
  theme_classic() +
  geom_smooth(method = "lm", se=FALSE)

tail_total 
tail_head
```


## Residuals, Sum of Squared Errors

The difference between what was observed in the data and what was predicted from the regression line is what we call an "error" or a “residual.” Observations that lie above the regression line exceeded their predicted value and have a _positive residual_. Observations that lie below the regression line are less than their predicted value and have a _negative residual_.

Recall that one of our assumptions needed in order to prove that OLS is **unbiased** and has the **lowest variance** of all unbiased estimators was that the population error $u$ is: normally distributed, has a mean of zero, and has constant variance. While these assumptions are not directly testable because we can never observe $u$, we can use our regression residuals $e$ to assess the plausibility of these assumptions in the population. 

**Exercise:**

Using the regression of total length on tail length:

1. Add to the `possum` dataframe a column of predicted total lengths $\hat y_i$ and a column of residuals $e_i = y_i-\hat y_i$.^[**Hint:** In `dplyr` you can use `add_predictions(mod)` where `mod` is a regression model to generate a new variable containing your model's predicted values.]

```{r}
regression <- lm(total_l ~ tail_l, data = possum)
predictions <- possum %>% 
  add_predictions(regression) %>% 
  mutate(residuals = total_l - pred)
```


2. Make a histogram of your residuals. Do they look approximately normally distributed?

```{r}
ggplot(predictions, aes(x = residuals)) +
  geom_histogram()
```


3. Compute the mean of your residuals. Is it basically zero?^[**Note:** OLS gives you this one for free. The OLS regression line will always go through the middle of your data, as it aims to minimize squared errors. So, you'll always end up with sample errors that are mean zero. This doesn't necessarily mean that in your population $u$ is mean zero!]

```{r}
mean(predictions$residuals)
```


4. Plot residuals against your independent variable, tail length. Does the variance look approximately constant across all values of tail length?

```{r}
ggplot(predictions, aes(x = tail_l, y = residuals)) +
  geom_point() +
  theme_classic()
```


## Coefficient of Determination in a Regression

We estimated a linear relationship between possum tail length and total possum length above, and we recovered OLS estimates of $\hat\beta_0$ and $\hat\beta_1$. From our correlation calculations, we also have a sense of the strength of these linear relationships. Here, we will use a concept that is very closely related to correlation to quantify the overall fit of our linear regression model. 

Recall that the coefficient of determination, or $R^2$, is the share of the variance in $y$ that is explained by your regression model. Defining SSR as the sum of squared residuals (sum of the square of all our prediction errors) and SST as the total sum of squares (proportional to variance of $y$), we have:

$$R^{2}=1-\frac{S S R}{S S T}=1-\frac{\operatorname{Var}(e)}{\operatorname{Var}(y)}$$

This is the most commonly cited measure of the fit of a regression model. $R^2$ ranges from 0 (my regression model explains none of the variation in $y$) to 1 (my regression model perfectly explains all variation in $y$).

**Exercise**

1. Use the function `summary()` after the regression to calculate the $R^2$ for both the total length and tail length relationship, _and_ the head length and tail length relationship. 

2. Which regression model fits the data better? Is this what you expected based on the correlations computed above? Why or why not?


